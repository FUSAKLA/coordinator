export const TimelineData = {
    "storeEvents": [
        {
            "id": "6000",
            "type": "Incident",
            "title": "planned outage: migration new interests tree",
            "start": "2021-01-27T18:43:38.303+01:00",
            "end": "2021-01-29T07:45:43.507+01:00",
            "description": "Step by step manual for all teams -> https://gitlab.seznam.net/se/ab1/infrastructure/-/issues/84\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/1784b592-b033-4eb2-8172-23525f9e36de-1611777014667/details\n\n## Error budget spent: \n* userportal:critical:availability: 15.27%\n* userportal:low:availability: 2.05%\n\n## Timeline\n - 2021-01-27 18:54:00 : incident started # ondrej.vlk\n - 2021-01-27 18:55:00 : start silence in alertmanager\"\n - 2021-01-27 18:57:00 : scale down userproxy in KO & NG to 0 replicas\"\n - 2021-01-27 19:47:00 : deploy adminservers 12.4.0 & apiserver-drak 9.0.8\"\n - 2021-01-27 20:17:00 : restart userweb-nas pods (scale to 0 and back)\"\n - 2021-01-27 20:30:00 : scale up userproxy in KO & NG to 3 replicas\"\n - 2021-01-27 20:47:00 : remove silence in alertmanager\"\n - 2021-01-27 20:48:00 : incident ended # ondrej.vlk\n\n\n## Corrective actions\n* documentation: https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/merge_requests/313\n\n## Silenced critical alerts:\n* `alertname=SloOneHourAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=1h` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloSixHourAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=6h` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloOneDayAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=1d` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloThreeDaysAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=3d` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloExporterUserproxySklikProductionUpAbsent` `team=sklik.devops@firma.seznam.cz`\n* ??? `alertname=SloSixHourAlert` `alert_type=slo:high_burnrate` `slo_class=high_fast` `slo_domain=partnerportal` `slo_time_range=6h` `slo_type=latency90` `team=sklik.prostor@firma.seznam.cz`\n\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [X] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [X] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [X] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\n<!--\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "martin.junk",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/a010e85c055b56d645b0e3b14ab4cab3?s=80&d=identicon"
            },
            "number_of_comments": 0
        },
        {
            "id": "5998",
            "type": "Incident",
            "title": "sspweb high latency",
            "start": "2021-01-27T10:21:48.896+01:00",
            "end": "2021-01-27T12:55:41.812+01:00",
            "description": "Short description here\n\nOpsgenie:\n* links here\n* second link\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n* (If more than 1% was spent)\n\n## Timeline\n - 2021-01-27 10:21:48 : Issue created # stanislav.mach\n\n\n## Graphs\n(insert graphs from the incident here)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "stanislav.mach",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/809/avatar.png"
            },
            "number_of_comments": 0
        },
        {
            "id": "5994",
            "type": "Incident",
            "title": "user-profile.ko at kokura has problem with sending alerts",
            "start": "2021-01-23T08:03:45.756+01:00",
            "end": "2021-01-25T10:06:01.782+01:00",
            "description": "user-profile.ko at kokura failed to send 2 alerts in last 10m to\n\n**Labels**:\n* **alertname**: PrometheusErrorSendingAlerts\n* **alert_type**: monitoring\n* **alertmanager_port**: 32528\n* **cluster**: user-profile.ko\n* **escalate**: sklik.sre\n* **locality**: kokura\n* **prometheus_type**: harvester\n* **severity**: :fire: critical\n* **team**: sklik.devops@firma.seznam.cz\n\n# WTF IS `cluster=user-profile.ko`\n\nNothing seems to be in Gitlab!\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/5c318ad3-5843-4249-8261-c6848be8fd35-1611381914170\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n(???)\n\n## Timeline\n - 2021-01-23 07:05:00 : page created\n - 2021-01-23 07:28:00 : page acknowledged # rudolf.thomas\n - 2021-01-23 07:59:00 : alert stopped firing\n - 2021-01-23 08:03:45 : Issue created # rudolf.thomas\n\n\n## Graphs\n(insert graphs from the incident here)\n\n## Corrective actions",
            "responsible_person": {
                "name": "rudolf.thomas",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/1453/avatar.png"
            },
            "number_of_comments": 0
        },
        {
            "id": "5993",
            "type": "Incident",
            "title": "user-profile.ko  at kokura has problem with sending alerts",
            "start": "2021-01-23T05:25:40.423+01:00",
            "end": "2021-01-26T10:06:48.437+01:00",
            "description": "Due to error in alertmanager silence, alert came form user-profile.ko cluster.\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/56c01f67-3eec-4426-8a4b-2eaac09130c0-1611371548119/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/b5d7440d-bb16-4da8-8118-29a06c810ab3-1611396648196/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/73e8699e-ea69-4963-82a0-de34c7d2ae32-1611401424121/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/dff471e2-1e11-4057-887f-bb34dc96b6d2-1611376364178/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/91028619-9299-48d9-83ee-ec09539f1be5-1611406078074/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/2b50366b-bf8d-4a3d-b4fc-cf8cee714362-1611407398106/details\n\nPostmortem: (if there is one)\n\n### Error budget spent: \n* (If more than 1% was spent)\n\n### Timeline\n - 2021-01-23 05:25:40 : Issue created # daniil.svirin\n\n### Graphs\n(insert graphs from the incident here)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n### Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.",
            "responsible_person": {
                "name": "daniil.svirin",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/68f639e02fe1e5b830606bfa0b8b0aba?s=80&d=identicon"
            },
            "number_of_comments": 0
        },
        {
            "id": "5992",
            "type": "Incident",
            "title": "High availability burn-rate in SLO domain partnerportal",
            "start": "2021-01-21T14:44:57.998+01:00",
            "end": "2021-01-31T11:19:49.961290935+01:00",
            "description": "Short description here\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/25928585-7d3e-44f4-9590-9a0d6dccda36-1611234328821/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/1956bc11-5cc7-4fe6-934c-b21063c65ebc-1611235814567/details\n\nPostmortem: (if there is one)\n\n### Error budget spent: \n* (If more than 1% was spent)\n\n### Timeline\n\n// * YYYY-MM-DD HH:MM - something happend\n\n### Graphs\n(insert graphs from the incident here)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n### Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [x] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.",
            "responsible_person": {
                "name": "jan.skrle",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/686/avatar.png"
            },
            "number_of_comments": 0
        },
        {
            "id": "5990",
            "type": "Incident",
            "title": "Adminserver is overloaded again",
            "start": "2021-01-20T12:22:58.438+01:00",
            "end": "2021-01-25T12:17:31.466+01:00",
            "description": "Short description here\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/7f0077f7-464a-447d-8e6a-7207a8e055b9-1611140850485/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/862eeee5-5d9b-432b-8e54-b7224648de6a-1611143550629/details\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n* (If more than 1% was spent)\n\n## Timeline\n - 2021-01-20 12:23:23 : Issue created # ivo.capoun\n\n## Logs:\nexport-worker\n```\n2021-01-19 14:52:36,724 ERROR: [1|Dummy-18144][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681603]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 14:52:36,742 ERROR: [1|Dummy-18144][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681603]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-19 15:05:07,677 ERROR: [1|Dummy-18180][6ee7586e5459e2f8:frontend-api:387483:435691-460822-7026470]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 15:05:07,694 ERROR: [1|Dummy-18180][6ee7586e5459e2f8:frontend-api:387483:435691-460822-7026470]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-19 14:52:36,708 ERROR: [1|Dummy-18201][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681591]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 14:52:36,726 ERROR: [1|Dummy-18201][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681591]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:24,800 ERROR: [1|Dummy-18437][fc60bccf3d70718a:frontend-api:362574:435871-428597-70295281]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-20 11:43:24,826 ERROR: [1|Dummy-18437][fc60bccf3d70718a:frontend-api:362574:435871-428597-70295281]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 12:18:22,267 ERROR: [1|Dummy-18548][83f8f6eb974cc0fa:frontend-api:184914:435880-2550035-108684645]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-20 12:18:22,288 ERROR: [1|Dummy-18548][83f8f6eb974cc0fa:frontend-api:184914:435880-2550035-108684645]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:24,686 ERROR: [1|Dummy-18609][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977961]: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='idserver', port=3356): Read timed out. (read timeout=3))\"} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='idserver', port=3356): Read timed out. (read timeout=3))\"}\n2021-01-20 11:43:24,703 ERROR: [1|Dummy-18609][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977961]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:25,569 ERROR: [1|Dummy-18654][fc60bccf3d70718a:frontend-api:362574:435871-906490-108538151]: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"}\n2021-01-20 11:43:25,585 ERROR: [1|Dummy-18654][fc60bccf3d70718a:frontend-api:362574:435871-906490-108538151]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 09:30:00,256 ERROR: [1|Dummy-17998][605275c7b3bdda23:frontend-api:356327:435826-2296022-107200705]: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"}\n2021-01-20 09:30:00,276 ERROR: [1|Dummy-17998][605275c7b3bdda23:frontend-api:356327:435826-2296022-107200705]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-19 15:05:07,852 ERROR: [1|Dummy-18217][6ee7586e5459e2f8:frontend-api:387483:435691-460819-7026393]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 15:05:07,869 ERROR: [1|Dummy-18217][6ee7586e5459e2f8:frontend-api:387483:435691-460819-7026393]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:24,574 ERROR: [1|Dummy-14086][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977953]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-20 11:43:24,587 ERROR: [1|Dummy-14086][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977953]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n\n```\n\n## Graphs\nadminserver\n![image](/uploads/32c91fcca99fcc26a86dd586858d4f7f/image.png)\n\nexport-worker \n![image](/uploads/5386e2bc89a80526c6bd066a18cf4ac6/image.png)\n\nzde je vidět odchozí volání export-workera, které je v pohodě\n![image](/uploads/7d2561132377e8af0fa72ab6c1038b00/image.png)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [x] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "ivo.capoun",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/bcb78983d6e55f9630f9a03ce50f6cf9?s=80&d=identicon"
            },
            "number_of_comments": 1
        },
        {
            "id": "5989",
            "type": "Incident",
            "title": "PostHog proxy requests are slow",
            "start": "2021-01-19T14:33:09.501+01:00",
            "end": "2021-01-22T10:05:52.687+01:00",
            "description": "Posthog was edited by hand by @lukas.svoboda2. The reason was to debugging latency (find out reasonable minimal instances in HPA).\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/e9d28c5d-774a-4722-ba38-513cfdcb503d-1611062002570\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n* (If more than 1% was spent)\n\nThere is no SLO for PostHog.\n\n## Timeline\n - 2021-01-19 14:13:00 : alert `PostHogProxyHighLatency` paged\n - 2021-01-19 14:33:09 : Issue created # rudolf.thomas\n - 2021-01-19 15:05:00 : increased maximum number of replicas (12 to 36) # rudolf.thomas\n\n## Logs\n\nAll going well until a connection refused is hit (connection to what?) then the pod got terminated (after about a second!):\n```\n10.64.94.169 - - [19/Jan/2021:13:29:41 +0000] \"POST /s/?compression=gzip-js&ip=1&_=1611062980587 HTTP/1.0\" 200 13 \"https://nas.sklik.cz/campaigns?table=(dir:DESC,limit:250,page:1,sort:impressions)&dateRange=(from:%272019-03-01%27,to:%272021-01-19%27)&status=nondeleted&segmentation=!()&filter=()\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0\"\n10.74.198.65 - - [19/Jan/2021:13:29:42 +0000] \"GET / HTTP/1.0\" 302 0 \"-\" \"Go-http-client/1.1\"\n10.64.94.169 - - [19/Jan/2021:13:29:42 +0000] \"POST /s/?compression=gzip-js&ip=1&_=1611062981915 HTTP/1.0\" 200 13 \"https://www.sklik.cz/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\nunexpected error ConnectionRefusedError(111, 'Connection refused') while sending data\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/statsd/connection.py\", line 77, in send\n    self.udp_sock.send(send_data)\nConnectionRefusedError: [Errno 111] Connection refused\nunexpected error ConnectionRefusedError(111, 'Connection refused') while sending data\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/statsd/connection.py\", line 77, in send\n    self.udp_sock.send(send_data)\nConnectionRefusedError: [Errno 111] Connection refused\n...\n```\n\nHorizontal Pod Autoscaler (HPA) is doing things:\n\n```\n$ k --context=kube1.ko --namespace=sklik-posthog-production describe horizontalpodautoscaler.autoscaling/posthog-web\n...\nMax replicas:                                          12\nDeployment pods:                                       12 current / 12 desired\nConditions:\n  Type            Status  Reason               Message\n  ----            ------  ------               -------\n  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation\n  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\n  ScalingLimited  True    TooManyReplicas      the desired replica count is more than the maximum replica count\nEvents:\n  Type    Reason             Age                   From                       Message\n  ----    ------             ----                  ----                       -------\n  Normal  SuccessfulRescale  42m (x56 over 5d22h)  horizontal-pod-autoscaler  New size: 3; reason: All metrics below target\n  Normal  SuccessfulRescale  41m (x34 over 6d3h)   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  25m                   horizontal-pod-autoscaler  New size: 8; reason: Current number of replicas below Spec.MinReplicas\n  Normal  SuccessfulRescale  15m                   horizontal-pod-autoscaler  New size: 9; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  9m59s                 horizontal-pod-autoscaler  New size: 8; reason: All metrics below target\n  Normal  SuccessfulRescale  6m56s                 horizontal-pod-autoscaler  New size: 12; reason: cpu resource utilization (percentage of request) above target\n```\n\nAs seen from the pod side:\n```\nk --context=kube1.ko --namespace=sklik-posthog-production get pods\n...\n  Normal  ScalingReplicaSet  26m                 deployment-controller  Scaled up replica set posthog-web-cb6d998b9 to 8\n  Normal  ScalingReplicaSet  22m                 deployment-controller  Scaled down replica set posthog-web-cb6d998b9 to 6\n  Normal  ScalingReplicaSet  22m                 deployment-controller  Scaled up replica set posthog-web-74df57d657 to 4\n  Normal  ScalingReplicaSet  22m                 deployment-controller  Scaled up replica set posthog-web-74df57d657 to 2\n  Normal  ScalingReplicaSet  21m                 deployment-controller  Scaled up replica set posthog-web-74df57d657 to 5\n  Normal  ScalingReplicaSet  19m (x2 over 179m)  deployment-controller  Scaled up replica set posthog-web-659f7c97fb to 4\n  Normal  ScalingReplicaSet  19m                 deployment-controller  Scaled down replica set posthog-web-74df57d657 to 1\n  Normal  ScalingReplicaSet  17m                 deployment-controller  Scaled up replica set posthog-web-cb6d998b9 to 6\n  Normal  ScalingReplicaSet  17m (x2 over 21m)   deployment-controller  Scaled down replica set posthog-web-cb6d998b9 to 5\n  Normal  ScalingReplicaSet  10m (x74 over 27h)  deployment-controller  (combined from similar events): Scaled down replica set posthog-web-cb6d998b9 to 3\n```\n\n## Graphs\n\nhttps://grafana.sklik.iszn.cz/goto/v3hR40BMk\n\n## Corrective actions\n\n- [x] fix the playbook link in the alert\n- [ ] the the posthog dashboard (some graphs magically ang wrongly change to 7 days when \"last 1 hour\" is set in the top-right corner)\n- [x] On-call hero must by informed about dangerous changes in production.",
            "responsible_person": {
                "name": "martin.chodur",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/542/avatar.png"
            },
            "number_of_comments": 0
        },
        {
            "id": "5988",
            "type": "Incident",
            "title": "High latency90 burn-rate in SLO domain partnerportal (last hour)",
            "start": "2021-01-19T10:21:46.063+01:00",
            "end": "2021-01-31T11:19:49.961294168+01:00",
            "description": "Short description here\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/2d7c2314-b6ec-415c-95e7-f2bb34f3809e-1611047348518/details\n\nPostmortem: (if there is one)\n\n### Error budget spent: \n* (If more than 1% was spent)\n\n### Timeline\n * 2021-01-19 10:09 - alert starts to fire\n\n### Graphs\n![Screenshot_2021-01-19_SLO_Drilldown_-_Grafana_1_](/uploads/e8e16fe26d89eed010e7999cbceb8286/Screenshot_2021-01-19_SLO_Drilldown_-_Grafana_1_.png)\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n### Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [x] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.",
            "responsible_person": {
                "name": "jan.skrle",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/686/avatar.png"
            },
            "number_of_comments": 0
        },
        {
            "id": "5981",
            "type": "Incident",
            "title": "High latency{70,90} burn-rate in SLO domain userportal-exports (6 hours, low traffic)",
            "start": "2021-01-14T09:12:30.819+01:00",
            "end": "2021-01-14T09:46:15.528+01:00",
            "description": "Short description here\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/a786a312-7bfb-4600-b080-7fa80a8215d7-1610547750453/logs\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/a786a312-7bfb-4600-b080-7fa80a8215d7-1610547750453/logs\n\nPostmortem: (if there is one)\n\n\n## Error budget spent: \n* less than 1%\n\n\n## Timeline\n - 2021-01-13 15:22:16 : An alert started to fire, snoozed by ondrej.vlk # frantisek.reznicek\n - 2021-01-14 09:12:30 : Issue created, evaluation started # frantisek.reznicek\n - \n\n\n## Graphs\nSLO drilldown (latency 70)\n![image](/uploads/cab8f26eb6fcc754ee7aa2362850d766/image.png)\n![image](/uploads/313057252a6ef908afdf2e3e9899edf5/image.png)\n\nSLO drilldown (latency 90)\n![image](/uploads/fc436daf8d3b5248d04dfdce6026e052/image.png)\n![image](/uploads/70ce4fbe2d4e468f4464696022c56b3e/image.png)\n\nExport-manager/worker status\n![image](/uploads/82f0b2de3635b74c561ee737b05893dd/image.png)\n\n\n\n\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\ngrafana_annotation_id: 0\n--><!--\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "frantisek.reznicek",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/617/avatar.png"
            },
            "number_of_comments": 1
        },
        {
            "id": "5976",
            "type": "Incident",
            "title": "high burnrate on userportal-exports",
            "start": "2021-01-13T15:24:44.349+01:00",
            "end": "2021-01-19T10:23:20.199+01:00",
            "description": "Possibly caused by overloaded adminserver -  see graphs\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/a786a312-7bfb-4600-b080-7fa80a8215d7-1610547750453/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/41230117-1229-48c2-b1ba-86831413a3e7-1610547670407/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/c6217654-e781-4a5c-9152-b3cad5b77705-1610620390458\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/cf0ebb62-654f-40f4-9ca7-7017d51a2803-1610628370588\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/3ff96339-3756-4d32-af66-c7db44a7269a-1610634110603/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/22c76c66-b0d9-4867-b987-c399ec99fa45-1610633430470/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/cf0ebb62-654f-40f4-9ca7-7017d51a2803-1610628370588/details\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n* userportal-exports:small:latency90: 9.17%\n* userportal-exports:large:latency70: 19.23%\n* userportal-exports:large:latency90: 38.46%\n* userportal-exports:medium:latency70: 8.26%\n* userportal-exports:medium:latency90: 24.77%\n* userportal-exports:small:latency70: 3.55%\n\n\n## Timeline\n - 2021-01-13 14:40:00 : incident started # ondrej.vlk\n - 2021-01-13 15:23:57 : Issue created # ondrej.vlk\n - 2021-01-14 15:38:00 : restarted all `adminserver` instances # ondrej.vlk\n - 2021-01-14 15:40:00 : incident ended # ondrej.vlk\n\n\n## Graphs\nAdminserver requests\n\n![Snímek_obrazovky_2021-01-13_15-38-54](/uploads/91ba0abce54f35b1a4f3d3dce5cce890/Snímek_obrazovky_2021-01-13_15-38-54.png)\n\n## Logs\n\new.log\n```\n2021-01-13 14:30:42,766 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Received task: export_group {worker.py:310}\n2021-01-13 14:30:42,853 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method user.getAttributes in 0.084 {client.py:194}\n2021-01-13 14:30:42,855 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method user.getAttributes to adminserver finish, call time: 0:00:00.086695 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:30:42,937 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method groups.get in 0.071 {client.py:194}\n2021-01-13 14:30:42,938 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method groups.get to adminserver finish, call time: 0:00:00.071737 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:30:42,997 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method keywords.list in 0.048 {client.py:194}\n2021-01-13 14:30:42,997 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method keywords.list to adminserver finish, call time: 0:00:00.049000 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:30:43,081 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method ads.list in 0.071 {client.py:194}\n2021-01-13 14:30:43,082 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method ads.list to adminserver finish, call time: 0:00:00.071526 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:30:43,152 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method ads.list in 0.059 {client.py:194}\n2021-01-13 14:30:43,152 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method ads.list to adminserver finish, call time: 0:00:00.060080 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:36,925 INFO: [1|MainThread][]: Started consuming from 434491 {consumer.py:471}\n2021-01-13 14:32:53,224 ERROR: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: HTTPConnectionPool(host='adminserver', port=3335): Max retries exceeded with url: /RPC2 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcb6e081860>: Failed to establish a new connection: [Errno 110] Connection timed out')) {retrying_proxy.py:115}\nTraceback (most recent call last):\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/connection.py\", line 170, in _new_conn\n    (self._dns_host, self.port), self.timeout, **extra_kw\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/util/connection.py\", line 96, in create_connection\n    raise err\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n    sock.connect(sa)\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/gevent/_socket3.py\", line 407, in connect\n    raise error(err, strerror(err))\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n    chunked=chunked,\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/sklik_urllib3_fastrpc_client/patch_urllib3.py\", line 38, in wrapper\n    result = make_request(self, *args, **kwargs)\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 394, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/connection.py\", line 234, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/usr/lib/python3.7/http/client.py\", line 1244, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/usr/lib/python3.7/http/client.py\", line 1290, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/usr/lib/python3.7/http/client.py\", line 1239, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/usr/lib/python3.7/http/client.py\", line 1026, in _send_output\n    self.send(msg)\n  File \"/usr/lib/python3.7/http/client.py\", line 966, in send\n    self.connect()\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/connection.py\", line 200, in connect\n    conn = self._new_conn()\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/connection.py\", line 182, in _new_conn\n    self, \"Failed to establish a new connection: %s\" % e\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fcb6e081860>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/app/export_worker/connectors/retrying_proxy.py\", line 92, in __call__\n    res = getattr(self.proxy.actual_proxy, self.partial_method_name)(*args)\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/sklik_urllib3_fastrpc_client/client.py\", line 191, in __call__\n    body=body\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/request.py\", line 79, in request\n    method, url, fields=fields, headers=headers, **urlopen_kw\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/request.py\", line 170, in request_encode_body\n    return self.urlopen(method, url, **extra_kw)\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n  File \"/root/.cache/pypoetry/virtualenvs/export-worker-9TtSrW0h-py3.7/lib/python3.7/site-packages/urllib3/util/retry.py\", line 573, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='adminserver', port=3335): Max retries exceeded with url: /RPC2 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcb6e081860>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n2021-01-13 14:32:53,256 ERROR: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-13 14:32:53,695 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method patterns.list in 0.025 {client.py:194}\n2021-01-13 14:32:53,696 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method patterns.list to adminserver finish, call time: 0:00:00.025604 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:53,751 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method patterns.negative.list in 0.046 {client.py:194}\n2021-01-13 14:32:53,752 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method patterns.negative.list to adminserver finish, call time: 0:00:00.046202 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:53,802 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method sitelink.list in 0.036 {client.py:194}\n2021-01-13 14:32:53,803 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method sitelink.list to adminserver finish, call time: 0:00:00.037051 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:53,822 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method retargeting.group.list in 0.007 {client.py:194}\n2021-01-13 14:32:53,822 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method retargeting.group.list to adminserver finish, call time: 0:00:00.008147 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:53,843 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method retargeting.group.negative.list in 0.011 {client.py:194}\n2021-01-13 14:32:53,843 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method retargeting.group.negative.list to adminserver finish, call time: 0:00:00.011830 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:53,894 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method ages.list in 0.043 {client.py:194}\n2021-01-13 14:32:53,894 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method ages.list to adminserver finish, call time: 0:00:00.043123 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:53,942 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method genders.list in 0.037 {client.py:194}\n2021-01-13 14:32:53,942 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method genders.list to adminserver finish, call time: 0:00:00.038087 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:53,978 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method interests.negative.list in 0.022 {client.py:194}\n2021-01-13 14:32:53,979 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method interests.negative.list to adminserver finish, call time: 0:00:00.022954 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:54,024 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method interests.list in 0.036 {client.py:194}\n2021-01-13 14:32:54,025 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method interests.list to adminserver finish, call time: 0:00:00.036404 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:54,066 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method themes.list in 0.032 {client.py:194}\n2021-01-13 14:32:54,067 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method themes.list to adminserver finish, call time: 0:00:00.032804 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:54,113 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method themes.negative.list in 0.030 {client.py:194}\n2021-01-13 14:32:54,113 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method themes.negative.list to adminserver finish, call time: 0:00:00.030653 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:54,159 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method productSets.list in 0.037 {client.py:194}\n2021-01-13 14:32:54,160 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method productSets.list to adminserver finish, call time: 0:00:00.037647 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:54,311 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method ads.list in 0.139 {client.py:194}\n2021-01-13 14:32:54,311 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method ads.list to adminserver finish, call time: 0:00:00.139590 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:54,376 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Finished adminserver call RPC method ads.list in 0.053 {client.py:194}\n2021-01-13 14:32:54,376 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Call method ads.list to adminserver finish, call time: 0:00:00.054016 [s], with FRPC status: 200 {retrying_proxy.py:101}\n2021-01-13 14:32:54,399 INFO: [1|Dummy-15742][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Task export_group finish with state SUCCESS in 131.62503531202674[s] {worker.py:341}\n```\n\n\nas.log\n```\n2021/01/13 14:32:53 I3: [96][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:53 I3: [96][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_node\" at sklik@skdb06.gong.seznam.cz established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:53 I3: [96][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'sitelink.list(283827, 283827, {\"includeDel...\": false, \"groupIds\": (...)})' status=200 client=127.0.0.1 time=30.860ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:53 I3: [100][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:53 I3: [100][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_node\" at sklik@skdb06.gong.seznam.cz established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:53 I3: [100][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'genders.list(283827, 283827, (107460185))' status=200 client=127.0.0.1 time=32.540ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:54 I3: [98][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:54 I3: [98][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_node\" at sklik@skdb06.gong.seznam.cz established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:54 I3: [98][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'themes.negative.list(283827, 283827, {\"groupIds\": (...), \"includeDel...\": false}, {\"includeThe...\": true})' status=200 client=127.0.0.1 time=25.415ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:30:43 I3: [98][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'ads.list(283827, 283827, {\"adTypes\": (...), \"groupIds\": (...)}, {\"showTracki...\": true, \"showApprov...\": true})' status=200 client=127.0.0.1 time=65.457ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:53 I3: [98][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'interests.negative.list(283827, 283827, {\"groupIds\": (...), \"includeDel...\": false})' status=200 client=127.0.0.1 time=17.406ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:54 I3: [97][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'productSets.list(283827, 283827, {\"groupIds\": (...)}, {\"showLabels...\": true})' status=200 client=127.0.0.1 time=32.461ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Encrypted password used to configure database. {node.py:__createConfig():329}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Creating SqlWrapper instance {sqlwrapper.py:__init__():160}\n2021/01/13 14:32:53 I4: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Running in master-only mode. {sqlwrapper.cc:init():165}\n2021/01/13 14:32:53 I3: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:53 I3: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_relation\" at sklik_sre_prod@skdbrel established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:53 I3: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:53 I3: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_node\" at sklik@skdb06.gong.seznam.cz established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:53 I3: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:53 I3: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_common\" at sklik@skdbcom.gong.seznam.cz established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:53 I3: [129][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'patterns.negative.list(283827, 283827, {\"groupIds\": (...)}, {})' status=200 client=127.0.0.1 time=40.620ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:53 I3: [100][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:53 I3: [100][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_node\" at sklik@skdb06.gong.seznam.cz established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:53 I3: [100][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'ages.list(283827, 283827, (107460185))' status=200 client=127.0.0.1 time=38.099ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:54 I3: [101][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Master db trying to set names with command SET NAMES utf8 {sqlwrapper.cc:connectMaster():346}\n2021/01/13 14:32:54 I3: [101][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: Connection to master MySQL database \"sklik_node\" at sklik@skdb06.gong.seznam.cz established {sqlwrapper.cc:connectMaster():358}\n2021/01/13 14:32:54 I3: [101][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'themes.list(283827, 283827, {\"groupIds\": (...), \"includeDel...\": false}, {\"includeThe...\": true})' status=200 client=127.0.0.1 time=27.398ms {pyfastrpcinterface.cc:postProcess():1240}\n2021/01/13 14:32:54 I3: [99][275663f9ccbca135:frontend-api:42850527:283827:434488-2401750-107460185]: adminserver 'ads.list(283827, 283827, {\"adTypes\": (...), \"groupIds\": (...)}, {\"showImageU...\": true, \"showImpres...\": true, \"showTracki...\": true, \"showImpres...\": true, \"showApprov...\": true, \"showImageI...\": true, \"showName\": true})' status=200 client=127.0.0.1 time=48.202ms {pyfastrpcinterface.cc:postProcess():1240}\n```\n\n\nSLO drilldown (latency 70)\n![image](/uploads/cab8f26eb6fcc754ee7aa2362850d766/image.png)\n![image](/uploads/313057252a6ef908afdf2e3e9899edf5/image.png)\n\nSLO drilldown (latency 90)\n![image](/uploads/fc436daf8d3b5248d04dfdce6026e052/image.png)\n![image](/uploads/70ce4fbe2d4e468f4464696022c56b3e/image.png)\n\nExport-manager/worker status\n![image](/uploads/82f0b2de3635b74c561ee737b05893dd/image.png)\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\ngrafana_annotation_id: 0\n-->\n<!--\ngrafana_annotation_id: 0\n--><!--\ngrafana_annotation_id: 0\n--><!--\ngrafana_annotation_id: 0\n--><!--\ngrafana_annotation_id: 0\n--><!--\ngrafana_annotation_id: 0\n--><!--\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "ivo.capoun",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/bcb78983d6e55f9630f9a03ce50f6cf9?s=80&d=identicon"
            },
            "number_of_comments": 5
        }
    ]
}
