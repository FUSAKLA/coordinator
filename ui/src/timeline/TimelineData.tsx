export const TimelineData = {
    "storeEvents": [
        {
            "id": "6000",
            "type": "Incident",
            "state": "Active",
            "title": "planned outage: migration new interests tree",
            "start": "2021-01-27T18:43:38.303+01:00",
            "end": "2021-01-29T07:45:43.507+01:00",
            "description": "Step by step manual for all teams -> https://gitlab.seznam.net/se/ab1/infrastructure/-/issues/84\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/1784b592-b033-4eb2-8172-23525f9e36de-1611777014667/details\n\n## Error budget spent: \n* userportal:critical:availability: 15.27%\n* userportal:low:availability: 2.05%\n\n## Timeline\n - 2021-01-27 18:54:00 : incident started # ondrej.vlk\n - 2021-01-27 18:55:00 : start silence in alertmanager\"\n - 2021-01-27 18:57:00 : scale down userproxy in KO & NG to 0 replicas\"\n - 2021-01-27 19:47:00 : deploy adminservers 12.4.0 & apiserver-drak 9.0.8\"\n - 2021-01-27 20:17:00 : restart userweb-nas pods (scale to 0 and back)\"\n - 2021-01-27 20:30:00 : scale up userproxy in KO & NG to 3 replicas\"\n - 2021-01-27 20:47:00 : remove silence in alertmanager\"\n - 2021-01-27 20:48:00 : incident ended # ondrej.vlk\n\n\n## Corrective actions\n* documentation: https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/merge_requests/313\n\n## Silenced critical alerts:\n* `alertname=SloOneHourAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=1h` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloSixHourAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=6h` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloOneDayAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=1d` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloThreeDaysAlert` `alert_type=slo:high_burnrate` `slo_class=critical` `slo_domain=userportal` `slo_time_range=3d` `slo_type=availability` `team=sklik.sre@firma.seznam.cz`\n* `alertname=SloExporterUserproxySklikProductionUpAbsent` `team=sklik.devops@firma.seznam.cz`\n* ??? `alertname=SloSixHourAlert` `alert_type=slo:high_burnrate` `slo_class=high_fast` `slo_domain=partnerportal` `slo_time_range=6h` `slo_type=latency90` `team=sklik.prostor@firma.seznam.cz`\n\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [X] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [X] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [X] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\n<!--\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "martin.junk",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/a010e85c055b56d645b0e3b14ab4cab3?s=80&d=identicon"
            },
            "number_of_comments": 0,
            "labels": ["Sklik"]
        },
        {
            "id": "5998",
            "type": "Maintenance",
            "state": "Active",
            "title": "sspweb high latency",
            "start": "2021-01-27T10:21:48.896+01:00",
            "end": "2021-01-27T12:55:41.812+01:00",
            "description": "Short description here\n\nOpsgenie:\n* links here\n* second link\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n* (If more than 1% was spent)\n\n## Timeline\n - 2021-01-27 10:21:48 : Issue created # stanislav.mach\n\n\n## Graphs\n(insert graphs from the incident here)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "stanislav.mach",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/809/avatar.png"
            },
            "number_of_comments": 0,
            "labels": ["OpenStack"]
        },
        {
            "id": "5994",
            "type": "Notice",
            "state": "Finished",
            "title": "user-profile.ko at kokura has problem with sending alerts",
            "start": "2021-01-23T08:03:45.756+01:00",
            "end": "2021-01-25T10:06:01.782+01:00",
            "description": "user-profile.ko at kokura failed to send 2 alerts in last 10m to\n\n**Labels**:\n* **alertname**: PrometheusErrorSendingAlerts\n* **alert_type**: monitoring\n* **alertmanager_port**: 32528\n* **cluster**: user-profile.ko\n* **escalate**: sklik.sre\n* **locality**: kokura\n* **prometheus_type**: harvester\n* **severity**: :fire: critical\n* **team**: sklik.devops@firma.seznam.cz\n\n# WTF IS `cluster=user-profile.ko`\n\nNothing seems to be in Gitlab!\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/5c318ad3-5843-4249-8261-c6848be8fd35-1611381914170\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n(???)\n\n## Timeline\n - 2021-01-23 07:05:00 : page created\n - 2021-01-23 07:28:00 : page acknowledged # rudolf.thomas\n - 2021-01-23 07:59:00 : alert stopped firing\n - 2021-01-23 08:03:45 : Issue created # rudolf.thomas\n\n\n## Graphs\n(insert graphs from the incident here)\n\n## Corrective actions",
            "responsible_person": {
                "name": "rudolf.thomas",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/1453/avatar.png"
            },
            "number_of_comments": 0,
            "labels": ["Kubernetes"]
        },
        {
            "id": "5993",
            "type": "Incident",
            "state": "Active",
            "title": "user-profile.ko  at kokura has problem with sending alerts",
            "start": "2021-01-23T05:25:40.423+01:00",
            "end": "2021-01-26T10:06:48.437+01:00",
            "description": "Due to error in alertmanager silence, alert came form user-profile.ko cluster.\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/56c01f67-3eec-4426-8a4b-2eaac09130c0-1611371548119/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/b5d7440d-bb16-4da8-8118-29a06c810ab3-1611396648196/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/73e8699e-ea69-4963-82a0-de34c7d2ae32-1611401424121/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/dff471e2-1e11-4057-887f-bb34dc96b6d2-1611376364178/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/91028619-9299-48d9-83ee-ec09539f1be5-1611406078074/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/2b50366b-bf8d-4a3d-b4fc-cf8cee714362-1611407398106/details\n\nPostmortem: (if there is one)\n\n### Error budget spent: \n* (If more than 1% was spent)\n\n### Timeline\n - 2021-01-23 05:25:40 : Issue created # daniil.svirin\n\n### Graphs\n(insert graphs from the incident here)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n### Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [ ] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.",
            "responsible_person": {
                "name": "daniil.svirin",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/68f639e02fe1e5b830606bfa0b8b0aba?s=80&d=identicon"
            },
            "number_of_comments": 0,
            "labels": []
        },
        {
            "id": "5992",
            "type": "Maintenance",
            "state": "Active",
            "title": "High availability burn-rate in SLO domain partnerportal",
            "start": "2021-01-21T14:44:57.998+01:00",
            "end": "2021-01-31T11:19:49.961290935+01:00",
            "description": "Short description here\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/25928585-7d3e-44f4-9590-9a0d6dccda36-1611234328821/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/1956bc11-5cc7-4fe6-934c-b21063c65ebc-1611235814567/details\n\nPostmortem: (if there is one)\n\n### Error budget spent: \n* (If more than 1% was spent)\n\n### Timeline\n\n// * YYYY-MM-DD HH:MM - something happend\n\n### Graphs\n(insert graphs from the incident here)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n### Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [x] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.",
            "responsible_person": {
                "name": "jan.skrle",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/686/avatar.png"
            },
            "number_of_comments": 0,
            "labels": []
        },
        {
            "id": "5990",
            "type": "Notice",
            "state": "Finished",
            "title": "Adminserver is overloaded again",
            "start": "2021-01-20T12:22:58.438+01:00",
            "end": "2021-01-25T12:17:31.466+01:00",
            "description": "Short description here\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/7f0077f7-464a-447d-8e6a-7207a8e055b9-1611140850485/details\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/862eeee5-5d9b-432b-8e54-b7224648de6a-1611143550629/details\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n* (If more than 1% was spent)\n\n## Timeline\n - 2021-01-20 12:23:23 : Issue created # ivo.capoun\n\n## Logs:\nexport-worker\n```\n2021-01-19 14:52:36,724 ERROR: [1|Dummy-18144][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681603]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 14:52:36,742 ERROR: [1|Dummy-18144][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681603]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-19 15:05:07,677 ERROR: [1|Dummy-18180][6ee7586e5459e2f8:frontend-api:387483:435691-460822-7026470]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 15:05:07,694 ERROR: [1|Dummy-18180][6ee7586e5459e2f8:frontend-api:387483:435691-460822-7026470]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-19 14:52:36,708 ERROR: [1|Dummy-18201][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681591]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 14:52:36,726 ERROR: [1|Dummy-18201][701fe55ba88ed7b0:frontend-api:84750154:358600:435679-2549120-108681591]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:24,800 ERROR: [1|Dummy-18437][fc60bccf3d70718a:frontend-api:362574:435871-428597-70295281]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-20 11:43:24,826 ERROR: [1|Dummy-18437][fc60bccf3d70718a:frontend-api:362574:435871-428597-70295281]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 12:18:22,267 ERROR: [1|Dummy-18548][83f8f6eb974cc0fa:frontend-api:184914:435880-2550035-108684645]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-20 12:18:22,288 ERROR: [1|Dummy-18548][83f8f6eb974cc0fa:frontend-api:184914:435880-2550035-108684645]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:24,686 ERROR: [1|Dummy-18609][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977961]: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='idserver', port=3356): Read timed out. (read timeout=3))\"} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='idserver', port=3356): Read timed out. (read timeout=3))\"}\n2021-01-20 11:43:24,703 ERROR: [1|Dummy-18609][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977961]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:25,569 ERROR: [1|Dummy-18654][fc60bccf3d70718a:frontend-api:362574:435871-906490-108538151]: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"}\n2021-01-20 11:43:25,585 ERROR: [1|Dummy-18654][fc60bccf3d70718a:frontend-api:362574:435871-906490-108538151]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 09:30:00,256 ERROR: [1|Dummy-17998][605275c7b3bdda23:frontend-api:356327:435826-2296022-107200705]: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': \"Server error: ReadTimeoutError(HTTPConnectionPool(host='access-server', port=3003): Read timed out. (read timeout=3))\"}\n2021-01-20 09:30:00,276 ERROR: [1|Dummy-17998][605275c7b3bdda23:frontend-api:356327:435826-2296022-107200705]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-19 15:05:07,852 ERROR: [1|Dummy-18217][6ee7586e5459e2f8:frontend-api:387483:435691-460819-7026393]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-19 15:05:07,869 ERROR: [1|Dummy-18217][6ee7586e5459e2f8:frontend-api:387483:435691-460819-7026393]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n2021-01-20 11:43:24,574 ERROR: [1|Dummy-14086][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977953]: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'} {retrying_proxy.py:115}\nexport_worker.connectors.retrying_proxy.FastrpcWrongResponse: {'status': 500, 'statusMessage': 'Internal server error: Failed access check call'}\n2021-01-20 11:43:24,587 ERROR: [1|Dummy-14086][fc60bccf3d70718a:frontend-api:362574:435871-906490-41977953]: Call to adminserver failed, retry iteration: 1, exponential retry coefficient: 0.2, retry after 0.4 [s] {retrying_proxy.py:132}\n\n```\n\n## Graphs\nadminserver\n![image](/uploads/32c91fcca99fcc26a86dd586858d4f7f/image.png)\n\nexport-worker \n![image](/uploads/5386e2bc89a80526c6bd066a18cf4ac6/image.png)\n\nzde je vidět odchozí volání export-workera, které je v pohodě\n![image](/uploads/7d2561132377e8af0fa72ab6c1038b00/image.png)\n\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n## Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [x] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.\n\ngrafana_annotation_id: 0\n-->",
            "responsible_person": {
                "name": "ivo.capoun",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/bcb78983d6e55f9630f9a03ce50f6cf9?s=80&d=identicon"
            },
            "number_of_comments": 1,
            "labels": []
        },
        {
            "id": "5989",
            "type": "Incident",
            "state": "Finished",
            "title": "PostHog proxy requests are slow",
            "start": "2021-01-19T14:33:09.501+01:00",
            "end": "2021-01-22T10:05:52.687+01:00",
            "description": "Posthog was edited by hand by @lukas.svoboda2. The reason was to debugging latency (find out reasonable minimal instances in HPA).\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/e9d28c5d-774a-4722-ba38-513cfdcb503d-1611062002570\n\nPostmortem: (if there is one)\n\n## Error budget spent: \n* (If more than 1% was spent)\n\nThere is no SLO for PostHog.\n\n## Timeline\n - 2021-01-19 14:13:00 : alert `PostHogProxyHighLatency` paged\n - 2021-01-19 14:33:09 : Issue created # rudolf.thomas\n - 2021-01-19 15:05:00 : increased maximum number of replicas (12 to 36) # rudolf.thomas\n\n## Logs\n\nAll going well until a connection refused is hit (connection to what?) then the pod got terminated (after about a second!):\n```\n10.64.94.169 - - [19/Jan/2021:13:29:41 +0000] \"POST /s/?compression=gzip-js&ip=1&_=1611062980587 HTTP/1.0\" 200 13 \"https://nas.sklik.cz/campaigns?table=(dir:DESC,limit:250,page:1,sort:impressions)&dateRange=(from:%272019-03-01%27,to:%272021-01-19%27)&status=nondeleted&segmentation=!()&filter=()\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0\"\n10.74.198.65 - - [19/Jan/2021:13:29:42 +0000] \"GET / HTTP/1.0\" 302 0 \"-\" \"Go-http-client/1.1\"\n10.64.94.169 - - [19/Jan/2021:13:29:42 +0000] \"POST /s/?compression=gzip-js&ip=1&_=1611062981915 HTTP/1.0\" 200 13 \"https://www.sklik.cz/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\nunexpected error ConnectionRefusedError(111, 'Connection refused') while sending data\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/statsd/connection.py\", line 77, in send\n    self.udp_sock.send(send_data)\nConnectionRefusedError: [Errno 111] Connection refused\nunexpected error ConnectionRefusedError(111, 'Connection refused') while sending data\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/statsd/connection.py\", line 77, in send\n    self.udp_sock.send(send_data)\nConnectionRefusedError: [Errno 111] Connection refused\n...\n```\n\nHorizontal Pod Autoscaler (HPA) is doing things:\n\n```\n$ k --context=kube1.ko --namespace=sklik-posthog-production describe horizontalpodautoscaler.autoscaling/posthog-web\n...\nMax replicas:                                          12\nDeployment pods:                                       12 current / 12 desired\nConditions:\n  Type            Status  Reason               Message\n  ----            ------  ------               -------\n  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation\n  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\n  ScalingLimited  True    TooManyReplicas      the desired replica count is more than the maximum replica count\nEvents:\n  Type    Reason             Age                   From                       Message\n  ----    ------             ----                  ----                       -------\n  Normal  SuccessfulRescale  42m (x56 over 5d22h)  horizontal-pod-autoscaler  New size: 3; reason: All metrics below target\n  Normal  SuccessfulRescale  41m (x34 over 6d3h)   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  25m                   horizontal-pod-autoscaler  New size: 8; reason: Current number of replicas below Spec.MinReplicas\n  Normal  SuccessfulRescale  15m                   horizontal-pod-autoscaler  New size: 9; reason: cpu resource utilization (percentage of request) above target\n  Normal  SuccessfulRescale  9m59s                 horizontal-pod-autoscaler  New size: 8; reason: All metrics below target\n  Normal  SuccessfulRescale  6m56s                 horizontal-pod-autoscaler  New size: 12; reason: cpu resource utilization (percentage of request) above target\n```\n\nAs seen from the pod side:\n```\nk --context=kube1.ko --namespace=sklik-posthog-production get pods\n...\n  Normal  ScalingReplicaSet  26m                 deployment-controller  Scaled up replica set posthog-web-cb6d998b9 to 8\n  Normal  ScalingReplicaSet  22m                 deployment-controller  Scaled down replica set posthog-web-cb6d998b9 to 6\n  Normal  ScalingReplicaSet  22m                 deployment-controller  Scaled up replica set posthog-web-74df57d657 to 4\n  Normal  ScalingReplicaSet  22m                 deployment-controller  Scaled up replica set posthog-web-74df57d657 to 2\n  Normal  ScalingReplicaSet  21m                 deployment-controller  Scaled up replica set posthog-web-74df57d657 to 5\n  Normal  ScalingReplicaSet  19m (x2 over 179m)  deployment-controller  Scaled up replica set posthog-web-659f7c97fb to 4\n  Normal  ScalingReplicaSet  19m                 deployment-controller  Scaled down replica set posthog-web-74df57d657 to 1\n  Normal  ScalingReplicaSet  17m                 deployment-controller  Scaled up replica set posthog-web-cb6d998b9 to 6\n  Normal  ScalingReplicaSet  17m (x2 over 21m)   deployment-controller  Scaled down replica set posthog-web-cb6d998b9 to 5\n  Normal  ScalingReplicaSet  10m (x74 over 27h)  deployment-controller  (combined from similar events): Scaled down replica set posthog-web-cb6d998b9 to 3\n```\n\n## Graphs\n\nhttps://grafana.sklik.iszn.cz/goto/v3hR40BMk\n\n## Corrective actions\n\n- [x] fix the playbook link in the alert\n- [ ] the the posthog dashboard (some graphs magically ang wrongly change to 7 days when \"last 1 hour\" is set in the top-right corner)\n- [x] On-call hero must by informed about dangerous changes in production.",
            "responsible_person": {
                "name": "martin.chodur",
                "email": "",
                "avatar_url": "https://gitlab.seznam.net/uploads/-/system/user/avatar/542/avatar.png"
            },
            "number_of_comments": 0,
            "labels": []
        },
        {
            "id": "5988",
            "type": "Incident",
            "state": "Finished",
            "title": "High latency90 burn-rate in SLO domain partnerportal (last hour)",
            "start": "2021-01-19T10:21:46.063+01:00",
            "end": "2021-01-31T11:19:49.961294168+01:00",
            "description": "Short description here\n\nOpsgenie:\n* https://sklik-sre.app.eu.opsgenie.com/alert/detail/2d7c2314-b6ec-415c-95e7-f2bb34f3809e-1611047348518/details\n\nPostmortem: (if there is one)\n\n### Error budget spent: \n* (If more than 1% was spent)\n\n### Timeline\n * 2021-01-19 10:09 - alert starts to fire\n\n### Graphs\n![Screenshot_2021-01-19_SLO_Drilldown_-_Grafana_1_](/uploads/e8e16fe26d89eed010e7999cbceb8286/Screenshot_2021-01-19_SLO_Drilldown_-_Grafana_1_.png)\n## Corrective actions\n\nList issues that have been created as corrective actions from this incident.\nList format:\n    - <Corrective action title> | ca: #<corrective action id>\n\n### Checklist\n[Workflow guideline](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/oncall/incident_processing.md):\n\n- [x] spend almost one minute trying understand the incident\n- [x] create this issue\n- [x] Write message to [#sklik-production](https://teams.szn.cz/sklik/channels/sklik-production-url) on mattermost (or use imctl which could help you to do so)\n- Between 10:00 am and 5:00 pm certain developers [can deploy to production](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/release-workflow.md), so you should consider to lock production in autoadmins to prevent new issues\n- consider delegating the incident to another team member when you are overloaded\n- **consider notifying vypadky-sklik@firma.seznam.cz** and via other channels*\n  [channels](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/doc/announce-a-deployment.md#communication-channels) when it is a \"big incident\"\n- download logs, make grafana screenshosts\n- fix the incident\n- Create or update existing [playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/tree/master/howto) with commands you used to fix problem (don't spend a lot of time, max 5 minutes is ok, add more info after postmortem is written)\n- [ ] fullfill this incident\n   - Tag error peak on sklik-default grafana board (or use imctl which could help you to do so)\n   - add links to Opsgenie alerts\n      ```markdown\n      Opsgenie:\n      * opsgenie first alert url\n      * opsgenie second alert url\n      * ...\n      ```\n   - update incident timeline\n   - spend error budgets in format `* domain:class:type value` (`* partnerportal:critical:availability 21.332047`\n        * using [error-budget-change](https://gitlab.seznam.net/Sklik-DevOps/golibs/error-budget-change)\n        * using [imctl](https://gitlab.seznam.net/sklik-devops-sre/imctl/)\n        * using [grafana dashboard](https://grafana.sklik.iszn.cz/d/tvWu1vRGz/slo-errorbudget-change?orgId=1)\n   - [ ] update incident labels:\n     - `team::<TEAM>` label for team who is responsible for fixing this incident (e.g. `team::a-team`)\n     - `!proxied` - we had to contact somebody else to resolve incident\n     - `!known-issue` - this incident already happend in the past and root cause was not fixed yet\n     - `!self-resolved` - alert resolved automatically without any intervention\n     - `outage` if there was an outage\n     - caused by components `caused_by::component` (e.g. `caused_by::idserver`, `caused_by::adminserver, `caused_by::monitoring`)\n     - for each affected slo domain add label `slo_domain:domain_name (e.g. ~slo_domain:userportal)\n     - ~SLO_improvement_needed if there is an outage with influece on our customers and SLO does not change\n   - [ ] create [postmortem](https://gitlab.seznam.net/sklik-devops-sre/playbooks/blob/master/howto/incident_processing.md#writing-a-postmortem)\n       (from [template](https://gitlab.seznam.net/sklik-devops-sre/infrastructure/-/blob/master/.gitlab/issue_templates/post_mortem.md)) issue and add `postmortem: #postmortemID` to this incidnet.\n       In rare cases it is too bureaucratic to create a postmortem (e.g. it's straightforward that the incident cannot happen again) - in this case add label ~\"postmortem::not-needed\". And you\n       can add corrective actions (e.g. merge requests, commits) directly to the incident.\n   - [ ] if this is a one-time incident that can be fixed right away, then add label ~\"postmortem::not-needed\" to this incident and add a section with corrective actions links (add this section between Postmortem and Error budget spent sections). Example:\n      ```\n      Corrective actions:\n      * Gitlab issue link\n      * YouTrack issue link\n      * Merge Request link\n      * ...\n      ```\n   - if you're not sure if you need to write a postmortem read [this playbook](https://gitlab.seznam.net/sklik-devops-sre/playbooks/-/blob/master/howto/postmortem.md)\n* If incident consumes more than 10% of error budget or if it's consumed completly, it's required to send email to `vypadky-sklik@firma.seznam.cz`.",
            "responsible_person": {
                "name": "fusakla",
                "email": "",
                "avatar_url": "https://secure.gravatar.com/avatar/5c1c18abeb0e8b4cdff82966ba7c94b1?s=80&d=identicon"
            },
            "number_of_comments": 0,
            "labels": []
        }
    ]
}
